{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook serves as a template with the initial steps we took to load the dataset. You can use it as a guideline for your own implementation. Please note that since the dataset is obtained from the NSD experiment, bounded for the Algonauts Project challenge, you\n",
        "may be asked at some point to provide an email address associated with a drive account to access the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S66qkIAM7Ynv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Tw4nzR3nk7"
      },
      "source": [
        "# 0.1 Install and import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgsIQI4W3nk_"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdCJnaRC3nlB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from nilearn import datasets\n",
        "from nilearn import plotting\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
        "from torchvision import transforms\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr as corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGK48NX3nlE"
      },
      "source": [
        "# 0.2 Access the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running the notebooks on Google Colab you can use the Challenge data located in a Google Drive public folder called algonauts_2023_tutorial_data (fill out this [form](https://docs.google.com/forms/d/e/1FAIpQLSehZkqZOUNk18uTjRTuLj7UYmRGz-OkdsU25AyO3Wm6iAb0VA/alreadyresponded) to get the folder link). This folder contains the unzipped data of each subject.\n",
        "\n",
        "Before running the code you need to select this folder and choose \"Add a shortcut to Drive\". This will create a shortcut (without copying or taking space) of the folder to a desired path, for instance, in your Google Drive, from which you can read the content after mounting using drive.mount().\n",
        "\n",
        "Finally, edit the data_dir variable below with the path on your Drive to the algonauts_2023_tutorial_data shortcut folder, and the parent_submission_dir variable with the path on your Drive where you wish to save the predicted test fMRI data."
      ],
      "metadata": {
        "id": "hE_3pXIF4G_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJo5Y4xw3nlE"
      },
      "outputs": [],
      "source": [
        " from google.colab import drive\n",
        "    drive.mount('/content/drive/', force_remount=True)\n",
        "    data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data'\n",
        "    parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xvVJYLo3nlF"
      },
      "source": [
        "# 0.3 Select GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBWsaplR3nlG"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaFpbM-83nlG"
      },
      "source": [
        "# 1 Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nk2wxVu3nlI"
      },
      "outputs": [],
      "source": [
        "# Choose one of the 8 subjects\n",
        "\n",
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MosGTyQn3nlK"
      },
      "source": [
        "# 1.1 Define paths\n",
        "We define some paths in our drive needed for loading and storing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V6HvcP43nlL"
      },
      "outputs": [],
      "source": [
        "class argObj:\n",
        "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "\n",
        "    self.subj = format(subj, '02')\n",
        "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "    self.parent_submission_dir = parent_submission_dir\n",
        "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "        'subj'+self.subj)\n",
        "\n",
        "    # create the submission directory if not existing\n",
        "    if not os.path.isdir(self.subject_submission_dir):\n",
        "        os.makedirs(self.subject_submission_dir)\n",
        "\n",
        "args = argObj(data_dir, parent_submission_dir, subj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGkSVjbF3nlN"
      },
      "source": [
        "# 1.2 Load the fMRI training data\n",
        "The fMRI data consists of two '.npy' files:\n",
        "\n",
        "    lh_training_fmri.npy: the left hemisphere (LH) fMRI data.\n",
        "    rh_training_fmri.npy: the right hemisphere (RH) fMRI data.\n",
        "\n",
        "Both files are 2-dimensional arrays with training stimulus images as rows and fMRI vertices as columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r962gBg43nlN"
      },
      "outputs": [],
      "source": [
        "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
        "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('LH training fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Training stimulus images × LH vertices)')\n",
        "\n",
        "print('\\nRH training fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Training stimulus images × RH vertices)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
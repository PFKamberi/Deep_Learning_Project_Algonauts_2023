{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhi7WYSR1US"
      },
      "source": [
        "#Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaaRd1ZVILTo",
        "outputId": "7a48e0a3-21d9-404c-9413-b10abb4b6dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn==0.9.2\n",
            "  Downloading nilearn-0.9.2-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (4.9.2)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.5.3)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->nilearn==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0->nilearn==0.9.2) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1OBk-m9R_Uu"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V7A5A3ru9Y-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from nilearn import datasets, plotting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ZSavyOSLl1"
      },
      "source": [
        "#Mount to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcs-uTq39gnF",
        "outputId": "3984ace5-21c8-420e-c498-1d4dcee38a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data'\n",
        "parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KrPcW7GSN3h"
      },
      "source": [
        "#Select device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C3ruJC89jUE",
        "outputId": "715fbd64-9775-44aa-9c6a-cf82136e0208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5GZ9xqnSSNX"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vxPoAjZp9jvw"
      },
      "outputs": [],
      "source": [
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L2umiPF29nda"
      },
      "outputs": [],
      "source": [
        "class argObj:\n",
        "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "\n",
        "    self.subj = format(subj, '02')\n",
        "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "    self.parent_submission_dir = parent_submission_dir\n",
        "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "        'subj'+self.subj)\n",
        "\n",
        "args = argObj(data_dir, parent_submission_dir, subj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_azz_FRN9rjL",
        "outputId": "13ad6db5-688d-44ce-eea4-c19d05f32c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RH training fMRI data shape:\n",
            "(9841, 20544)\n",
            "(Training stimulus images × RH vertices)\n",
            "\n",
            "LH training fMRI data shape:\n",
            "(9841, 19004)\n",
            "(Training stimulus images × LH vertices)\n"
          ]
        }
      ],
      "source": [
        "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "\n",
        "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('\\nRH training fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Training stimulus images × RH vertices)')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
        "\n",
        "print('\\nLH training fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Training stimulus images × LH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yi6y52o9uOb",
        "outputId": "30b2e157-a5fb-492e-ca11-1e134a73bfc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 9841\n",
            "Test images: 159\n"
          ]
        }
      ],
      "source": [
        "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "train_img_list = os.listdir(train_img_dir)\n",
        "train_img_list.sort()\n",
        "test_img_list = os.listdir(test_img_dir)\n",
        "test_img_list.sort()\n",
        "print('Training images: ' + str(len(train_img_list)))\n",
        "print('Test images: ' + str(len(test_img_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LM6Fv0VkRF"
      },
      "source": [
        "#Train Validation and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PneuXQq9wVr",
        "outputId": "d1f5a0ff-3479-4242-c9a4-69ed16ff73af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stimulus images: 8857\n",
            "\n",
            "Validation stimulus images: 984\n",
            "\n",
            "Test stimulus images: 159\n"
          ]
        }
      ],
      "source": [
        "rand_seed = 5\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
        "# Shuffle all training stimulus images\n",
        "idxs = np.arange(len(train_img_list))\n",
        "np.random.shuffle(idxs)\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
        "# No need to shuffle or split the test stimulus images\n",
        "idxs_test = np.arange(len(test_img_list))\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmWbeAvOV2hr"
      },
      "source": [
        "#Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rqbsvQqGTT23"
      },
      "outputs": [],
      "source": [
        "# Define the custom dataset\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, imgs_paths, idxs, transform):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(device)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7UvgeNfTgvD"
      },
      "source": [
        "#Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Dbd1DJ9z20",
        "outputId": "a291a55f-8b98-48bf-9584-10aedea2276f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the transform for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize the images to 224x224 pixels\n",
        "    transforms.ToTensor(),  # convert the images to a PyTorch tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the images color channels\n",
        "])\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "class LinearizingEncodingModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim1, hidden_dim2, activation1, activation2,\n",
        "                 bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2):\n",
        "        super(LinearizingEncodingModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        if activation1:\n",
        "            self.activation1 = activation1()\n",
        "        if bnorm1:\n",
        "            self.batchnorm1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        if dropout1:\n",
        "            self.dropout1 = nn.Dropout(dropout_ratio1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        if activation2:\n",
        "            self.activation2 = activation2()\n",
        "        if bnorm2:\n",
        "            self.batchnorm2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        if dropout2:\n",
        "            self.dropout2 = nn.Dropout(dropout_ratio2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) #initial\n",
        "        if hasattr(self, 'activation1'):\n",
        "            x = self.activation1(x) #initial\n",
        "        if hasattr(self, 'batchnorm1'):\n",
        "            x = self.batchnorm1(x)\n",
        "        if hasattr(self, 'dropout1'):\n",
        "            x = self.dropout1(x)\n",
        "        x = self.fc2(x) #initial\n",
        "        if hasattr(self, 'activation2'):\n",
        "            x = self.activation2(x)\n",
        "        if hasattr(self, 'batchnorm2'):\n",
        "            x = self.batchnorm2(x)\n",
        "        if hasattr(self, 'dropout2'):\n",
        "            x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnwMwyubT3Tk"
      },
      "source": [
        "#Load Trained Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5_tSH1n84G1",
        "outputId": "aeddde78-f373-4da3-f37a-c8acd1d1f631"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_val = lh_fmri[idxs_val]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_val = rh_fmri[idxs_val]\n",
        "\n",
        "# Get the output shape of the feature extractor layer\n",
        "with torch.no_grad():\n",
        "    sample_input = torch.zeros(1, 3, 224, 224).to(device)\n",
        "    output = feature_extractor(sample_input)\n",
        "\n",
        "#The hyperparameter values where selected using optuna\n",
        "input_dim = output.shape[1]  # Set the dimensions for input and output of thr pretrained model\n",
        "output_dim = rh_fmri_train.shape[1]\n",
        "hidden_dim1 = 398\n",
        "hidden_dim2 = 130\n",
        "num_epochs = 50\n",
        "activation1 = nn.Tanh\n",
        "activation2 = nn.ReLU\n",
        "bnorm1 = False\n",
        "bnorm2 = False\n",
        "dropout1 = True\n",
        "dropout_ratio1 = 0.36525276151080455\n",
        "dropout2 = False\n",
        "dropout_ratio2 = 0.0\n",
        "learning_rate = 0.006044678842679579\n",
        "optimizer = optim.Adam\n",
        "loss_function = nn.MSELoss()\n",
        "batch_size = 150\n",
        "weight_decay = 0.0007265659377076724\n",
        "\n",
        "network_right = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/resnet50_right_hemishpere.pt', map_location=device)\n",
        "network_right.load_state_dict(checkpoint)\n",
        "\n",
        "output_dim = lh_fmri_train.shape[1]\n",
        "network_left = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/resnet50_left_hemishpere.pt', map_location=device)\n",
        "network_left.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnY9LZsj0Pq"
      },
      "source": [
        "#Make Predictions on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nY2wYMhdjzlO"
      },
      "outputs": [],
      "source": [
        "test_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_test, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_features = []\n",
        "    for data in test_imgs_dataloader:\n",
        "        inputs = data.to(device)\n",
        "        inputs = feature_extractor(inputs)\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "        test_features.append(inputs)\n",
        "    test_features = torch.cat(test_features, dim=0)\n",
        "\n",
        "np.save(\"resnet50_test_features.npy\", np.array(test_features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iYZZWDvKj5Vz"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "lh_fmri_test = lh_fmri[idxs_test]\n",
        "with torch.no_grad():\n",
        "    network_left.eval()\n",
        "    lh_fmri_test_pred = network_left(test_features)\n",
        "\n",
        "lh_correlation = np.zeros(lh_fmri_test_pred.shape[1])\n",
        "for v in range(lh_fmri_test_pred.shape[1]):\n",
        "    lh_correlation[v] = pearsonr(lh_fmri_test_pred[:, v].detach().cpu().numpy(), lh_fmri_test[:, v])[0]\n",
        "\n",
        "rh_fmri_test = rh_fmri[idxs_test]\n",
        "with torch.no_grad():\n",
        "    network_right.eval()\n",
        "    rh_fmri_test_pred = network_right(test_features)\n",
        "\n",
        "rh_correlation = np.zeros(rh_fmri_test_pred.shape[1])\n",
        "for v in range(rh_fmri_test_pred.shape[1]):\n",
        "    rh_correlation[v] = pearsonr(rh_fmri_test_pred[:, v].detach().cpu().numpy(), rh_fmri_test[:, v])[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUb596OlBNpb",
        "outputId": "daf8efa1-80d0-494d-ccab-d976ee677215"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14262287, 0.3032777 , 0.39484403, ..., 0.13386706, 0.21856263,\n",
              "       0.16511646], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of the fMRI image responses of all vertices on a brain surface map"
      ],
      "metadata": {
        "id": "Op4FyzkR7bXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 25 #@param\n",
        "hemisphere = 'left'\n",
        "\n",
        "if img < 0 or img > 158:\n",
        "    print(\"Please select a number between 1 and 159\")\n",
        "\n",
        "\n",
        "# Load the image\n",
        "img_dir = os.path.join(train_img_dir, train_img_list[idxs_test[img]])\n",
        "train_img = Image.open(img_dir).convert('RGB')\n",
        "\n",
        "# Plot the image\n",
        "\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Test image: ' + str(img));\n",
        "\n",
        "# Load the brain surface map of all vertices\n",
        "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
        "fsaverage_all_vertices = np.load(roi_dir)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_all_vertices))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = lh_fmri_test_pred[img]\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = rh_fmri_test_pred[img]\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title='All vertices, '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "dRs11_YW69cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of the fMRI image responses of a chosen ROI on a brain surface map"
      ],
      "metadata": {
        "id": "NMynHQlF7qhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 25 #@param\n",
        "hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
        "roi = \"EBA\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "# Load the image\n",
        "img_dir = os.path.join(train_img_dir, train_img_list[idxs_test[img]])\n",
        "train_img = Image.open(img_dir).convert('RGB')\n",
        "\n",
        "# Plot the image\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Test image: ' + str(img));\n",
        "\n",
        "# Define the ROI class based on the selected ROI\n",
        "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "    roi_class = 'prf-visualrois'\n",
        "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "    roi_class = 'floc-bodies'\n",
        "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "    roi_class = 'floc-faces'\n",
        "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "    roi_class = 'floc-places'\n",
        "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "    roi_class = 'floc-words'\n",
        "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "    roi_class = 'streams'\n",
        "\n",
        "# Load the ROI brain surface maps\n",
        "challenge_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "fsaverage_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    'mapping_'+roi_class+'.npy')\n",
        "challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "# Select the vertices corresponding to the ROI of interest\n",
        "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        lh_fmri_test_pred.cpu().detach().numpy()[img,np.where(challenge_roi)[0]]\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        rh_fmri_test_pred.cpu().detach().numpy()[img,np.where(challenge_roi)[0]]\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title=roi+', '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "7TKmetcT7CM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
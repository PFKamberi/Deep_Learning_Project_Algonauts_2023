{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhi7WYSR1US"
      },
      "source": [
        "#Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaaRd1ZVILTo"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1OBk-m9R_Uu"
      },
      "source": [
        "#Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V7A5A3ru9Y-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from nilearn import datasets, plotting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ZSavyOSLl1"
      },
      "source": [
        "#Mount to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcs-uTq39gnF",
        "outputId": "3ecc5276-3db3-4165-d0e8-bd07c6be7d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data'\n",
        "parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KrPcW7GSN3h"
      },
      "source": [
        "#Select device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C3ruJC89jUE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5GZ9xqnSSNX"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vxPoAjZp9jvw"
      },
      "outputs": [],
      "source": [
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L2umiPF29nda"
      },
      "outputs": [],
      "source": [
        "class argObj:\n",
        "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "\n",
        "    self.subj = format(subj, '02')\n",
        "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "    self.parent_submission_dir = parent_submission_dir\n",
        "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "        'subj'+self.subj)\n",
        "\n",
        "args = argObj(data_dir, parent_submission_dir, subj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_azz_FRN9rjL"
      },
      "outputs": [],
      "source": [
        "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "\n",
        "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('\\nRH training fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Training stimulus images × RH vertices)')\n",
        "\n",
        "lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
        "\n",
        "print('\\nLH training fMRI data shape:')\n",
        "print(lh_fmri.shape)\n",
        "print('(Training stimulus images × LH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yi6y52o9uOb"
      },
      "outputs": [],
      "source": [
        "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "train_img_list = os.listdir(train_img_dir)\n",
        "train_img_list.sort()\n",
        "test_img_list = os.listdir(test_img_dir)\n",
        "test_img_list.sort()\n",
        "print('Training images: ' + str(len(train_img_list)))\n",
        "print('Test images: ' + str(len(test_img_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LM6Fv0VkRF"
      },
      "source": [
        "#Train Validation and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PneuXQq9wVr"
      },
      "outputs": [],
      "source": [
        "rand_seed = 5\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
        "# Shuffle all training stimulus images\n",
        "idxs = np.arange(len(train_img_list))\n",
        "np.random.shuffle(idxs)\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
        "# No need to shuffle or split the test stimulus images\n",
        "idxs_test = np.arange(len(test_img_list))\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmWbeAvOV2hr"
      },
      "source": [
        "#Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rqbsvQqGTT23"
      },
      "outputs": [],
      "source": [
        "# Define the custom dataset\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, imgs_paths, idxs, transform):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(device)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7UvgeNfTgvD"
      },
      "source": [
        "#Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Dbd1DJ9z20"
      },
      "outputs": [],
      "source": [
        "# Define the transform for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize the images to 224x224 pixels\n",
        "    transforms.ToTensor(),  # convert the images to a PyTorch tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the images color channels\n",
        "])\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "class LinearizingEncodingModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim1, hidden_dim2, activation1, activation2,\n",
        "                 bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2):\n",
        "        super(LinearizingEncodingModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        if activation1:\n",
        "            self.activation1 = activation1()\n",
        "        if bnorm1:\n",
        "            self.batchnorm1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        if dropout1:\n",
        "            self.dropout1 = nn.Dropout(dropout_ratio1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        if activation2:\n",
        "            self.activation2 = activation2()\n",
        "        if bnorm2:\n",
        "            self.batchnorm2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        if dropout2:\n",
        "            self.dropout2 = nn.Dropout(dropout_ratio2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) #initial\n",
        "        if hasattr(self, 'activation1'):\n",
        "            x = self.activation1(x) #initial\n",
        "        if hasattr(self, 'batchnorm1'):\n",
        "            x = self.batchnorm1(x)\n",
        "        if hasattr(self, 'dropout1'):\n",
        "            x = self.dropout1(x)\n",
        "        x = self.fc2(x) #initial\n",
        "        if hasattr(self, 'activation2'):\n",
        "            x = self.activation2(x)\n",
        "        if hasattr(self, 'batchnorm2'):\n",
        "            x = self.batchnorm2(x)\n",
        "        if hasattr(self, 'dropout2'):\n",
        "            x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnwMwyubT3Tk"
      },
      "source": [
        "#Load Trained Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5_tSH1n84G1"
      },
      "outputs": [],
      "source": [
        "\n",
        "lh_fmri_train = lh_fmri[idxs_train]\n",
        "lh_fmri_val = lh_fmri[idxs_val]\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_val = rh_fmri[idxs_val]\n",
        "\n",
        "# Get the output shape of the feature extractor layer\n",
        "with torch.no_grad():\n",
        "    sample_input = torch.zeros(1, 3, 224, 224).to(device)\n",
        "    output = feature_extractor(sample_input)\n",
        "\n",
        "#The hyperparameter values where selected using optuna\n",
        "input_dim = output.shape[1]  # Set the dimensions for input and output of thr pretrained model\n",
        "output_dim = rh_fmri_train.shape[1]\n",
        "hidden_dim1 = 398\n",
        "hidden_dim2 = 130\n",
        "num_epochs = 50\n",
        "activation1 = nn.Tanh\n",
        "activation2 = nn.ReLU\n",
        "bnorm1 = False\n",
        "bnorm2 = False\n",
        "dropout1 = True\n",
        "dropout_ratio1 = 0.36525276151080455\n",
        "dropout2 = False\n",
        "dropout_ratio2 = 0.0\n",
        "learning_rate = 0.006044678842679579\n",
        "optimizer = optim.Adam\n",
        "loss_function = nn.MSELoss()\n",
        "batch_size = 150\n",
        "weight_decay = 0.0007265659377076724\n",
        "\n",
        "network_right = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/resnet50_right_hemishpere.pt', map_location=device)\n",
        "network_right.load_state_dict(checkpoint)\n",
        "\n",
        "output_dim = lh_fmri_train.shape[1]\n",
        "network_left = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/resnet50_left_hemishpere.pt', map_location=device)\n",
        "network_left.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWnY9LZsj0Pq"
      },
      "source": [
        "#Make Predictions on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''saved_array = np.load(\"/content/drive/MyDrive/resnet50_test_features.npy\")\n",
        "\n",
        "test_features = torch.from_numpy(saved_array)'''\n",
        "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
        "test_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_test, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_features = []\n",
        "    for data in test_imgs_dataloader:\n",
        "        inputs = data.to(device)\n",
        "        inputs = feature_extractor(inputs)\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "        test_features.append(inputs)\n",
        "    test_features = torch.cat(test_features, dim=0)"
      ],
      "metadata": {
        "id": "gDfuOz9pIFRS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iYZZWDvKj5Vz"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "lh_fmri_test = lh_fmri[idxs_test]\n",
        "with torch.no_grad():\n",
        "    network_left.eval()\n",
        "    lh_fmri_test_pred = network_left(test_features)\n",
        "\n",
        "lh_correlation = np.zeros(lh_fmri_test_pred.shape[1])\n",
        "for v in range(lh_fmri_test_pred.shape[1]):\n",
        "    lh_correlation[v] = pearsonr(lh_fmri_test_pred[:, v].detach().cpu().numpy(), lh_fmri_test[:, v])[0]\n",
        "\n",
        "rh_fmri_test = rh_fmri[idxs_test]\n",
        "with torch.no_grad():\n",
        "    network_right.eval()\n",
        "    rh_fmri_test_pred = network_right(test_features)\n",
        "\n",
        "rh_correlation = np.zeros(rh_fmri_test_pred.shape[1])\n",
        "for v in range(rh_fmri_test_pred.shape[1]):\n",
        "    rh_correlation[v] = pearsonr(rh_fmri_test_pred[:, v].detach().cpu().numpy(), rh_fmri_test[:, v])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of the fMRI image responses of all vertices on a brain surface map"
      ],
      "metadata": {
        "id": "Op4FyzkR7bXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 43 #@param\n",
        "hemisphere = 'left'\n",
        "\n",
        "if img < 0 or img > 158:\n",
        "    print(\"Please select a number between 1 and 159\")\n",
        "\n",
        "\n",
        "# Load the image\n",
        "img_dir = os.path.join(train_img_dir, train_img_list[idxs_test[img]])\n",
        "train_img = Image.open(img_dir).convert('RGB')\n",
        "\n",
        "# Plot the image\n",
        "\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Test image: ' + str(img));\n",
        "\n",
        "# Load the brain surface map of all vertices\n",
        "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
        "fsaverage_all_vertices = np.load(roi_dir)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_all_vertices))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = lh_fmri_test_pred[img]\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = rh_fmri_test_pred[img]\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title='All vertices, '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "dRs11_YW69cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualization of the fMRI image responses of a chosen ROI on a brain surface map"
      ],
      "metadata": {
        "id": "NMynHQlF7qhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 90 #@param\n",
        "hemisphere = 'left' #@param ['left', 'right'] {allow-input: true}\n",
        "roi = \"EBA\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "# Load the image\n",
        "img_dir = os.path.join(train_img_dir, train_img_list[idxs_test[img]])\n",
        "train_img = Image.open(img_dir).convert('RGB')\n",
        "\n",
        "# Plot the image\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(train_img)\n",
        "plt.title('Test image: ' + str(img));\n",
        "\n",
        "# Define the ROI class based on the selected ROI\n",
        "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "    roi_class = 'prf-visualrois'\n",
        "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "    roi_class = 'floc-bodies'\n",
        "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "    roi_class = 'floc-faces'\n",
        "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "    roi_class = 'floc-places'\n",
        "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "    roi_class = 'floc-words'\n",
        "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "    roi_class = 'streams'\n",
        "\n",
        "# Load the ROI brain surface maps\n",
        "challenge_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "fsaverage_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    'mapping_'+roi_class+'.npy')\n",
        "challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "# Select the vertices corresponding to the ROI of interest\n",
        "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        lh_fmri_test_pred.cpu().detach().numpy()[img,np.where(challenge_roi)[0]]\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        rh_fmri_test_pred.cpu().detach().numpy()[img,np.where(challenge_roi)[0]]\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title=roi+', '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "7TKmetcT7CM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict brain response on any image"
      ],
      "metadata": {
        "id": "XgS180SEJmC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization of the fMRI image responses of all vertices on a brain surface map"
      ],
      "metadata": {
        "id": "L370uxR7JyYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/demokritos.jpg' #@param\n",
        "hemisphere = 'left' #@param ['left', 'right']\n",
        "\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "transformed_image = transform(img).to(device)\n",
        "with torch.no_grad():\n",
        "    inputs = transformed_image.unsqueeze(0)  # Add an extra dimension for batch size\n",
        "    inputs = inputs.to(device)\n",
        "    inputs = feature_extractor(inputs)\n",
        "    inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    network_right.eval()\n",
        "    rh_fmri_test_pred1 = network_right(inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "    network_left.eval()\n",
        "    lh_fmri_test_pred1 = network_left(inputs)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "\n",
        "\n",
        "# Load the brain surface map of all vertices\n",
        "roi_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.all-vertices_fsaverage_space.npy')\n",
        "fsaverage_all_vertices = np.load(roi_dir)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_all_vertices))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = lh_fmri_test_pred1\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_all_vertices)[0]] = rh_fmri_test_pred1\n",
        "\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title='All vertices, '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "UUsD7ScPtrWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/demokritos.jpg' #@param\n",
        "hemisphere = 'left' #@param ['left', 'right']\n",
        "roi = \"EBA\" #@param [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\", \"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\", \"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\", \"OPA\", \"PPA\", \"RSC\", \"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\", \"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"] {allow-input: true}\n",
        "\n",
        "\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "transformed_image = transform(img).to(device)\n",
        "with torch.no_grad():\n",
        "    inputs = transformed_image.unsqueeze(0)  # Add an extra dimension for batch size\n",
        "    inputs = inputs.to(device)\n",
        "    inputs = feature_extractor(inputs)\n",
        "    inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    network_right.eval()\n",
        "    rh_fmri_test_pred1 = network_right(inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "    network_left.eval()\n",
        "    lh_fmri_test_pred1 = network_left(inputs)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(img)\n",
        "\n",
        "\n",
        "# Define the ROI class based on the selected ROI\n",
        "if roi in [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"V3v\", \"V3d\", \"hV4\"]:\n",
        "    roi_class = 'prf-visualrois'\n",
        "elif roi in [\"EBA\", \"FBA-1\", \"FBA-2\", \"mTL-bodies\"]:\n",
        "    roi_class = 'floc-bodies'\n",
        "elif roi in [\"OFA\", \"FFA-1\", \"FFA-2\", \"mTL-faces\", \"aTL-faces\"]:\n",
        "    roi_class = 'floc-faces'\n",
        "elif roi in [\"OPA\", \"PPA\", \"RSC\"]:\n",
        "    roi_class = 'floc-places'\n",
        "elif roi in [\"OWFA\", \"VWFA-1\", \"VWFA-2\", \"mfs-words\", \"mTL-words\"]:\n",
        "    roi_class = 'floc-words'\n",
        "elif roi in [\"early\", \"midventral\", \"midlateral\", \"midparietal\", \"ventral\", \"lateral\", \"parietal\"]:\n",
        "    roi_class = 'streams'\n",
        "\n",
        "# Load the ROI brain surface maps\n",
        "challenge_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_challenge_space.npy')\n",
        "fsaverage_roi_class_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    hemisphere[0]+'h.'+roi_class+'_fsaverage_space.npy')\n",
        "roi_map_dir = os.path.join(args.data_dir, 'roi_masks',\n",
        "    'mapping_'+roi_class+'.npy')\n",
        "challenge_roi_class = np.load(challenge_roi_class_dir)\n",
        "fsaverage_roi_class = np.load(fsaverage_roi_class_dir)\n",
        "roi_map = np.load(roi_map_dir, allow_pickle=True).item()\n",
        "\n",
        "# Select the vertices corresponding to the ROI of interest\n",
        "roi_mapping = list(roi_map.keys())[list(roi_map.values()).index(roi)]\n",
        "challenge_roi = np.asarray(challenge_roi_class == roi_mapping, dtype=int)\n",
        "fsaverage_roi = np.asarray(fsaverage_roi_class == roi_mapping, dtype=int)\n",
        "\n",
        "# Map the fMRI data onto the brain surface map\n",
        "fsaverage_response = np.zeros(len(fsaverage_roi))\n",
        "if hemisphere == 'left':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        lh_fmri_test_pred1.cpu().detach().numpy()[0, np.where(challenge_roi)[0]]\n",
        "elif hemisphere == 'right':\n",
        "    fsaverage_response[np.where(fsaverage_roi)[0]] = \\\n",
        "        rh_fmri_test_pred1.cpu().detach().numpy()[0, np.where(challenge_roi)[0]]\n",
        "\n",
        "# Create the interactive brain surface map\n",
        "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
        "view = plotting.view_surf(\n",
        "    surf_mesh=fsaverage['infl_'+hemisphere],\n",
        "    surf_map=fsaverage_response,\n",
        "    bg_map=fsaverage['sulc_'+hemisphere],\n",
        "    threshold=1e-14,\n",
        "    cmap='cold_hot',\n",
        "    colorbar=True,\n",
        "    title=roi+', '+hemisphere+' hemisphere'\n",
        "    )\n",
        "view"
      ],
      "metadata": {
        "id": "axc6LbBdKHQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
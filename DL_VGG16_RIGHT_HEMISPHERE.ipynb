{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install necessary packages"
      ],
      "metadata": {
        "id": "NLhi7WYSR1US"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaaRd1ZVILTo"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn==0.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries"
      ],
      "metadata": {
        "id": "L1OBk-m9R_Uu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V7A5A3ru9Y-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from nilearn import datasets, plotting\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount to drive"
      ],
      "metadata": {
        "id": "o8ZSavyOSLl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcs-uTq39gnF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data'\n",
        "parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Select device"
      ],
      "metadata": {
        "id": "_KrPcW7GSN3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C3ruJC89jUE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import data"
      ],
      "metadata": {
        "id": "A5GZ9xqnSSNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vxPoAjZp9jvw"
      },
      "outputs": [],
      "source": [
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "L2umiPF29nda"
      },
      "outputs": [],
      "source": [
        "class argObj:\n",
        "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "\n",
        "    self.subj = format(subj, '02')\n",
        "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "    self.parent_submission_dir = parent_submission_dir\n",
        "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "        'subj'+self.subj)\n",
        "\n",
        "args = argObj(data_dir, parent_submission_dir, subj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_azz_FRN9rjL"
      },
      "outputs": [],
      "source": [
        "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('\\nRH training fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Training stimulus images Ã— RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yi6y52o9uOb"
      },
      "outputs": [],
      "source": [
        "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "train_img_list = os.listdir(train_img_dir)\n",
        "train_img_list.sort()\n",
        "test_img_list = os.listdir(test_img_dir)\n",
        "test_img_list.sort()\n",
        "print('Training images: ' + str(len(train_img_list)))\n",
        "print('Test images: ' + str(len(test_img_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Validation and Test Split"
      ],
      "metadata": {
        "id": "v5LM6Fv0VkRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PneuXQq9wVr"
      },
      "outputs": [],
      "source": [
        "rand_seed = 5\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
        "# Shuffle all training stimulus images\n",
        "idxs = np.arange(len(train_img_list))\n",
        "np.random.shuffle(idxs)\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
        "# No need to shuffle or split the test stimulus images\n",
        "idxs_test = np.arange(len(test_img_list))\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader"
      ],
      "metadata": {
        "id": "BmWbeAvOV2hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom dataset\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, imgs_paths, idxs, transform):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(device)\n",
        "        return img"
      ],
      "metadata": {
        "id": "rqbsvQqGTT23"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning and Training Loop"
      ],
      "metadata": {
        "id": "P7UvgeNfTgvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Dbd1DJ9z20"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the transform for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize the images to 224x224 pixels\n",
        "    transforms.ToTensor(),  # convert the images to a PyTorch tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the images color channels\n",
        "])\n",
        "\n",
        "# Remove the last layer of the pretrained model\n",
        "model = models.vgg19(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "class LinearizingEncodingModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim1, hidden_dim2, activation1, activation2,\n",
        "                 bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2):\n",
        "        super(LinearizingEncodingModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        if activation1:\n",
        "            self.activation1 = activation1()\n",
        "        if bnorm1:\n",
        "            self.batchnorm1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        if dropout1:\n",
        "            self.dropout1 = nn.Dropout(dropout_ratio1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        if activation2:\n",
        "            self.activation2 = activation2()\n",
        "        if bnorm2:\n",
        "            self.batchnorm2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        if dropout2:\n",
        "            self.dropout2 = nn.Dropout(dropout_ratio2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) #initial\n",
        "        if hasattr(self, 'activation1'):\n",
        "            x = self.activation1(x) #initial\n",
        "        if hasattr(self, 'batchnorm1'):\n",
        "            x = self.batchnorm1(x)\n",
        "        if hasattr(self, 'dropout1'):\n",
        "            x = self.dropout1(x)\n",
        "        x = self.fc2(x) #initial\n",
        "        if hasattr(self, 'activation2'):\n",
        "            x = self.activation2(x)\n",
        "        if hasattr(self, 'batchnorm2'):\n",
        "            x = self.batchnorm2(x)\n",
        "        if hasattr(self, 'dropout2'):\n",
        "            x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_linearizing_encoding_model(network, train_dataloader, train_targets, val_dataloader, val_targets, num_epochs,\n",
        "                                     batch_size, loss_function, optimizer, learning_rate, weight_decay, save_name=None, patience=3):\n",
        "    criterion = loss_function\n",
        "    optimizer = optimizer(network.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    network.train()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        training_loss = 0.0\n",
        "        for index, data in enumerate(train_dataloader):\n",
        "\n",
        "            inputs = data.to(device)\n",
        "            inputs = feature_extractor(inputs)\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "            targets_batch = torch.tensor(train_targets[index*batch_size : index*batch_size + batch_size if index+batch_size <= train_targets.shape[0] else train_targets.shape[0]-index*batch_size]).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = network(inputs)\n",
        "            loss = criterion(outputs, targets_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            training_loss += loss.item()\n",
        "\n",
        "        training_loss /= len(train_dataloader)\n",
        "        train_losses.append(training_loss)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {training_loss:.4f}')\n",
        "\n",
        "        network.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for index, data in enumerate(val_dataloader):\n",
        "                inputs = data.to(device)\n",
        "                inputs = feature_extractor(inputs)\n",
        "                inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "                targets_batch = torch.tensor(val_targets[index * batch_size: (index + 1) * batch_size]).to(device)\n",
        "\n",
        "                outputs = network(inputs)\n",
        "                loss = criterion(outputs, targets_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Validation - Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Check if the current validation loss is the best so far\n",
        "        if round(val_loss, 2) < round(best_val_loss, 2):\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Check if early stopping criterion is met\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(f'Early stopping triggered. No improvement in {patience} epochs.')\n",
        "            break\n",
        "\n",
        "    if save_name:\n",
        "        torch.save(network.state_dict(), save_name+'.pt')\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training"
      ],
      "metadata": {
        "id": "AnwMwyubT3Tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5_tSH1n84G1"
      },
      "outputs": [],
      "source": [
        "#Train and validation targets\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_val = rh_fmri[idxs_val]\n",
        "\n",
        "\n",
        "# Get the output shape of the feature extractor layer\n",
        "with torch.no_grad():\n",
        "    sample_input = torch.zeros(1, 3, 224, 224).to(device)\n",
        "    output = feature_extractor(sample_input)\n",
        "\n",
        "#The hyperparameter values where selected using optuna\n",
        "input_dim = output.flatten().shape[0]  # Set the dimensions for input and output of thr pretrained model\n",
        "output_dim = rh_fmri_train.shape[1]\n",
        "hidden_dim1 = 6487\n",
        "hidden_dim2 = 711\n",
        "num_epochs = 50\n",
        "activation1 = nn.ReLU\n",
        "activation2 = nn.Tanh\n",
        "bnorm1 = True\n",
        "bnorm2 = True\n",
        "dropout1 = False\n",
        "dropout_ratio1 = 0.4666007846830111\n",
        "dropout2 = True\n",
        "dropout_ratio2 = 0.098405552999689\n",
        "learning_rate = 0.008417049414747052\n",
        "optimizer = optim.SGD\n",
        "weight_decay = 0.07356043913788857\n",
        "loss_function = nn.MSELoss()\n",
        "batch_size = 40\n",
        "\n",
        "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
        "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
        "train_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_train, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_val, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "network = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "train_losses, val_losses = train_linearizing_encoding_model(network, train_imgs_dataloader, rh_fmri_train, val_imgs_dataloader, rh_fmri_val, num_epochs, batch_size, loss_function, optimizer,  learning_rate, weight_decay, \"vgg16_right_hemishpere\", 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning Curves"
      ],
      "metadata": {
        "id": "zVXtYiKiUCUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQJGtR8vOsed"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.savefig('vgg16_learning_curves_right_hemishpere.pdf', format='pdf')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaaRd1ZVILTo",
        "outputId": "ac7a2041-03bc-4666-ecc3-2d0c080b9eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn==0.9.2\n",
            "  Downloading nilearn-0.9.2-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (4.9.2)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.5.3)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->nilearn==0.9.2) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2->nilearn==0.9.2) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->nilearn==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0->nilearn==0.9.2) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.9.2\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.16)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn==0.9.2\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V7A5A3ru9Y-R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from nilearn import datasets\n",
        "from nilearn import plotting\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
        "from torchvision import transforms\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.stats import pearsonr as corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcs-uTq39gnF",
        "outputId": "a651f62b-16bb-47ef-ae8f-5d414ca56861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data'\n",
        "parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C3ruJC89jUE",
        "outputId": "bc439b4c-e9fb-4e91-902f-16826237ea59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vxPoAjZp9jvw"
      },
      "outputs": [],
      "source": [
        "subj = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L2umiPF29nda"
      },
      "outputs": [],
      "source": [
        "class argObj:\n",
        "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "\n",
        "    self.subj = format(subj, '02')\n",
        "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "    self.parent_submission_dir = parent_submission_dir\n",
        "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "        'subj'+self.subj)\n",
        "\n",
        "    # Create the submission directory if not existing\n",
        "    if not os.path.isdir(self.subject_submission_dir):\n",
        "        os.makedirs(self.subject_submission_dir)\n",
        "\n",
        "args = argObj(data_dir, parent_submission_dir, subj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_azz_FRN9rjL",
        "outputId": "c4b1339e-8566-4992-e9ad-9581b7b92510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RH training fMRI data shape:\n",
            "(9841, 20544)\n",
            "(Training stimulus images × RH vertices)\n"
          ]
        }
      ],
      "source": [
        "fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "print('\\nRH training fMRI data shape:')\n",
        "print(rh_fmri.shape)\n",
        "print('(Training stimulus images × RH vertices)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yi6y52o9uOb",
        "outputId": "7e36b0fd-446e-4985-85a2-c516eabb3ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 9841\n",
            "Test images: 159\n"
          ]
        }
      ],
      "source": [
        "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
        "\n",
        "# Create lists will all training and test image file names, sorted\n",
        "train_img_list = os.listdir(train_img_dir)\n",
        "train_img_list.sort()\n",
        "test_img_list = os.listdir(test_img_dir)\n",
        "test_img_list.sort()\n",
        "print('Training images: ' + str(len(train_img_list)))\n",
        "print('Test images: ' + str(len(test_img_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PneuXQq9wVr",
        "outputId": "df893d9f-72ab-4b9d-95de-6e6bdfe54f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training stimulus images: 8857\n",
            "\n",
            "Validation stimulus images: 984\n",
            "\n",
            "Test stimulus images: 159\n"
          ]
        }
      ],
      "source": [
        "rand_seed = 5 #@param\n",
        "np.random.seed(rand_seed)\n",
        "\n",
        "# Calculate how many stimulus images correspond to 90% of the training data\n",
        "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
        "# Shuffle all training stimulus images\n",
        "idxs = np.arange(len(train_img_list))\n",
        "np.random.shuffle(idxs)\n",
        "# Assign 90% of the shuffled stimulus images to the training partition,\n",
        "# and 10% to the test partition\n",
        "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
        "# No need to shuffle or split the test stimulus images\n",
        "idxs_test = np.arange(len(test_img_list))\n",
        "\n",
        "print('Training stimulus images: ' + format(len(idxs_train)))\n",
        "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
        "print('\\nTest stimulus images: ' + format(len(idxs_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Dbd1DJ9z20",
        "outputId": "701b7656-5548-4b86-a95f-cae1fb76f263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 339MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import optuna\n",
        "\n",
        "# Define the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transform for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize the images to 224x224 pixels\n",
        "    transforms.ToTensor(),  # convert the images to a PyTorch tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the images color channels\n",
        "])\n",
        "\n",
        "# Define the custom dataset\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, imgs_paths, idxs, transform):\n",
        "        self.imgs_paths = np.array(imgs_paths)[idxs]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs_paths[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img).to(device)\n",
        "        return img\n",
        "\n",
        "\n",
        "# Define the model and feature extractor\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "\n",
        "\n",
        "# Define the neural network for dimensionality reduction\n",
        "class LinearizingEncodingModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim1, hidden_dim2, activation1, activation2,\n",
        "                 bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2):\n",
        "        super(LinearizingEncodingModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) #initial\n",
        "        if activation1:\n",
        "            self.activation1 = activation1() #initial\n",
        "        if bnorm1:\n",
        "            self.batchnorm1 = nn.BatchNorm1d(hidden_dim1)\n",
        "        if dropout1:\n",
        "            self.dropout1 = nn.Dropout(dropout_ratio1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2) #initially with output dim\n",
        "        if activation2:\n",
        "            self.activation2 = activation2()\n",
        "        if bnorm2:\n",
        "            self.batchnorm2 = nn.BatchNorm1d(hidden_dim2)\n",
        "        if dropout2:\n",
        "            self.dropout2 = nn.Dropout(dropout_ratio2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x) #initial\n",
        "        if hasattr(self, 'activation1'):\n",
        "            x = self.activation1(x) #initial\n",
        "        if hasattr(self, 'batchnorm1'):\n",
        "            x = self.batchnorm1(x)\n",
        "        if hasattr(self, 'dropout1'):\n",
        "            x = self.dropout1(x)\n",
        "        x = self.fc2(x) #initial\n",
        "        if hasattr(self, 'activation2'):\n",
        "            x = self.activation2(x)\n",
        "        if hasattr(self, 'batchnorm2'):\n",
        "            x = self.batchnorm2(x)\n",
        "        if hasattr(self, 'dropout2'):\n",
        "            x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the dimensionality reduction network\n",
        "def train_linearizing_encoding_model(network, train_dataloader, train_targets, val_dataloader, val_targets, num_epochs, batch_size, loss_function, optimizer, learning_rate, save_name=None):\n",
        "    criterion = loss_function\n",
        "    optimizer = optimizer(network.parameters(), lr=learning_rate)\n",
        "    network.train()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        training_loss = 0.0\n",
        "        for index, data in enumerate(train_dataloader):\n",
        "\n",
        "\n",
        "\n",
        "            inputs = data.to(device)\n",
        "            inputs = feature_extractor(inputs)\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "\n",
        "            #targets = torch.zeros(inputs.size(0), output_dim).to(device)  # Change target dimension to output_dim\n",
        "            targets_batch = torch.tensor(train_targets[index*batch_size : index*batch_size + batch_size if index+batch_size <= train_targets.shape[0] else train_targets.shape[0]-index*batch_size]).to(device)\n",
        "            #print(index*batch_size, index*batch_size + batch_size)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = network(inputs)\n",
        "            loss = criterion(outputs, targets_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            training_loss += loss.item()\n",
        "\n",
        "        training_loss /= len(train_dataloader)\n",
        "        train_losses.append(training_loss)\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {training_loss:.4f}')\n",
        "\n",
        "        network.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for index, data in enumerate(val_dataloader):\n",
        "                inputs = data.to(device)\n",
        "                inputs = feature_extractor(inputs)\n",
        "                inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "                targets_batch = torch.tensor(val_targets[index * batch_size: (index + 1) * batch_size]).to(device)\n",
        "\n",
        "                outputs = network(inputs)\n",
        "                loss = criterion(outputs, targets_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Validation - Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    if save_name:\n",
        "        torch.save(network.state_dict(), save_name+'.pt')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5_tSH1n84G1",
        "outputId": "0c030743-d0b9-4254-8a31-502b49bb4f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dimension: 2048\n",
            "Epoch 1/2, Training Loss: 0.4516\n",
            "Validation - Epoch 1/2, Validation Loss: 0.4288\n",
            "Epoch 2/2, Training Loss: 0.4234\n",
            "Validation - Epoch 2/2, Validation Loss: 0.4459\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get the output shape of the feature extractor layer\n",
        "with torch.no_grad():\n",
        "    sample_input = torch.zeros(1, 3, 224, 224).to(device)\n",
        "    output = feature_extractor(sample_input)\n",
        "\n",
        "\n",
        "print(f'Input dimension: {output.flatten().shape[0]}')\n",
        "\n",
        "rh_fmri_train = rh_fmri[idxs_train]\n",
        "rh_fmri_val = rh_fmri[idxs_val]\n",
        "\n",
        "# Set the dimensions for input and output of the dimensionality reduction network\n",
        "input_dim = output.shape[1]  # Use the output shape of the feature extractor\n",
        "#input_dim = 512  # Dimension based on the ResNet18 architecture\n",
        "output_dim = rh_fmri_train.shape[1]  # Desired output dimension\n",
        "hidden_dim1 = 958 #256  # Number of hidden units in the network\n",
        "hidden_dim2 = 458\n",
        "num_epochs = 50\n",
        "activation1 = nn.Tanh\n",
        "activation2 = nn.Tanh\n",
        "bnorm1 = True\n",
        "bnorm2 = False\n",
        "dropout1 = False\n",
        "dropout_ratio1 = 0.0\n",
        "dropout2 = True\n",
        "dropout_ratio2 = 0.0\n",
        "learning_rate = 0.0007265659377076724\n",
        "optimizer = optim.Adam\n",
        "loss_function = nn.MSELoss()\n",
        "batch_size = 150\n",
        "\n",
        "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
        "test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
        "train_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_train, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_val, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "    # Train the dimensionality reduction network\n",
        "network = LinearizingEncodingModel(input_dim, output_dim, hidden_dim1,  hidden_dim2, activation1, activation2, bnorm1, bnorm2, dropout1, dropout_ratio1, dropout2, dropout_ratio2).to(device)\n",
        "train_losses, val_losses = train_linearizing_encoding_model(network, train_imgs_dataloader, rh_fmri_train, val_imgs_dataloader, rh_fmri_val, num_epochs, batch_size, loss_function, optimizer,  learning_rate, \"resnet50_right_hemishpere\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "TQJGtR8vOsed",
        "outputId": "12d8f662-dfbf-413c-f006-2afe1b372c72"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.savefig('resnet50_learning_curves_right_hemishpere.pdf', format='pdf')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SYkkRuo-S6zp"
      },
      "outputs": [],
      "source": [
        "val_imgs_dataloader = DataLoader(\n",
        "    ImageDataset(train_imgs_paths, idxs_val, transform),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    validation_features = []\n",
        "    for data in val_imgs_dataloader:\n",
        "        inputs = data.to(device)\n",
        "        inputs = feature_extractor(inputs)\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "        validation_features.append(inputs)\n",
        "    validation_features = torch.cat(validation_features, dim=0)\n",
        "\n",
        "rh_fmri_val = rh_fmri[idxs_val]\n",
        "with torch.no_grad():\n",
        "    network.eval()\n",
        "    rh_fmri_val_pred = network(validation_features)\n",
        "\n",
        "rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n",
        "for v in range(rh_fmri_val_pred.shape[1]):\n",
        "    rh_correlation[v] = pearsonr(rh_fmri_val_pred[:, v].detach().cpu().numpy(), rh_fmri_val[:, v])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqUIzPzlSFHd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = rh_fmri_val - rh_fmri_val_pred.cpu().detach().numpy()\n",
        "\n",
        "# Plot the residual plot with thinner dots\n",
        "plt.scatter(rh_fmri_val_pred.cpu().detach().numpy(), residuals, s=10)  # Adjust the size (s) parameter to make the dots thinner\n",
        "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.savefig('resnet50_residual_plot_right_hemishpere.pdf', format='pdf')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}